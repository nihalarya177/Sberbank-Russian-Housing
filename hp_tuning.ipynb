{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e18217a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as stats\n",
    "#import statsmodels.api as sm\n",
    "from math import ceil\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import SelectPercentile as SP\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from scipy.special import boxcox, inv_boxcox\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import math\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d391c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('X_train_final.csv')\n",
    "test_df = pd.read_csv('X_test_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd48d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_lambda = 0.21963574771308522\n",
    "target = train_df['price_doc']\n",
    "target_boxcox = train_df['price_doc_boxcox']\n",
    "train_df = train_df.drop(['price_doc', 'price_doc_boxcox'],axis = 1)\n",
    "#train_df = train_df.join(target)\n",
    "train_df = train_df.join(target_boxcox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e635430",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop('id',axis = 1)\n",
    "test_df = test_df.drop('id',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e346097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def reduce_mem_usage(df, verbose=True):\\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\\n    start_mem = df.memory_usage().sum() / 1024**2    \\n    for col in df.columns:\\n        col_type = df[col].dtypes\\n        if col_type in numerics:\\n            c_min = df[col].min()\\n            c_max = df[col].max()\\n            if str(col_type)[:3] == 'int':\\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\\n                    df[col] = df[col].astype(np.int8)\\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\\n                    df[col] = df[col].astype(np.int16)\\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\\n                    df[col] = df[col].astype(np.int32)\\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\\n                    df[col] = df[col].astype(np.int64)  \\n            else:\\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\\n                    df[col] = df[col].astype(np.float16)\\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\\n                    df[col] = df[col].astype(np.float32)\\n                else:\\n                    df[col] = df[col].astype(np.float64)    \\n    end_mem = df.memory_usage().sum() / 1024**2\\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\\n    return df\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4de7ec37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train= reduce_mem_usage(train_df)\\ntest= reduce_mem_usage(test_df)\\nprint(\"Shape of train set: \",train.shape)\\nprint(\"Shape of test set: \",test.shape)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''train= reduce_mem_usage(train_df)\n",
    "test= reduce_mem_usage(test_df)\n",
    "print(\"Shape of train set: \",train.shape)\n",
    "print(\"Shape of test set: \",test.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9234dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, mean_squared_error\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e45b2d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.iloc[:,:-1]\n",
    "y = train_df.iloc[:,-1]\n",
    "num_trials = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4760b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_objective(trial,data=X,target=y):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X , y, test_size=0.30, random_state=42)\n",
    "    param = {\n",
    "        'metric': 'rmse', \n",
    "        'random_state': 42,\n",
    "        'n_estimators': 20000,\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.005,0.008,0.01,0.03,0.05]),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11]),\n",
    "        'num_leaves' : trial.suggest_int('num_leaves', 1, 1000),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n",
    "        'cat_smooth' : trial.suggest_int('min_data_per_groups', 1, 100)\n",
    "    }\n",
    "    model = LGBMRegressor(**param)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],verbose=False)\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    rmse = mean_squared_error(y_test, preds,squared=False)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ab011eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-21 23:53:06,749]\u001b[0m A new study created in memory with name: no-name-e20e601d-b428-451d-8076-35424299f7fe\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-21 23:54:13,343]\u001b[0m Trial 0 finished with value: 13.065104845015327 and parameters: {'reg_alpha': 0.016944520944809544, 'reg_lambda': 0.005403607709884522, 'colsample_bytree': 1.0, 'subsample': 0.6, 'learning_rate': 0.005, 'max_depth': 7, 'num_leaves': 565, 'min_child_samples': 234, 'min_data_per_groups': 14}. Best is trial 0 with value: 13.065104845015327.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-21 23:55:52,568]\u001b[0m Trial 1 finished with value: 13.53277349538605 and parameters: {'reg_alpha': 2.2055220899028662, 'reg_lambda': 0.007065012213505477, 'colsample_bytree': 0.7, 'subsample': 0.8, 'learning_rate': 0.01, 'max_depth': 11, 'num_leaves': 864, 'min_child_samples': 230, 'min_data_per_groups': 61}. Best is trial 0 with value: 13.065104845015327.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-21 23:57:27,253]\u001b[0m Trial 2 finished with value: 13.434743189710977 and parameters: {'reg_alpha': 9.157893445339539, 'reg_lambda': 0.0027093416582697704, 'colsample_bytree': 0.4, 'subsample': 0.4, 'learning_rate': 0.008, 'max_depth': 11, 'num_leaves': 272, 'min_child_samples': 170, 'min_data_per_groups': 99}. Best is trial 0 with value: 13.065104845015327.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-21 23:58:09,870]\u001b[0m Trial 3 finished with value: 12.874837014299866 and parameters: {'reg_alpha': 0.1917218044930305, 'reg_lambda': 0.17870954033162642, 'colsample_bytree': 0.4, 'subsample': 0.7, 'learning_rate': 0.005, 'max_depth': 5, 'num_leaves': 992, 'min_child_samples': 199, 'min_data_per_groups': 17}. Best is trial 3 with value: 12.874837014299866.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-21 23:58:54,919]\u001b[0m Trial 4 finished with value: 13.663548815483534 and parameters: {'reg_alpha': 4.317124260201686, 'reg_lambda': 9.865685922733844, 'colsample_bytree': 0.5, 'subsample': 0.7, 'learning_rate': 0.03, 'max_depth': 5, 'num_leaves': 854, 'min_child_samples': 256, 'min_data_per_groups': 72}. Best is trial 3 with value: 12.874837014299866.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-22 00:00:28,192]\u001b[0m Trial 5 finished with value: 13.885032995069773 and parameters: {'reg_alpha': 0.002389427421042918, 'reg_lambda': 0.013864497927558303, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.05, 'max_depth': 9, 'num_leaves': 622, 'min_child_samples': 138, 'min_data_per_groups': 10}. Best is trial 3 with value: 12.874837014299866.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-22 00:01:49,748]\u001b[0m Trial 6 finished with value: 13.378006670486496 and parameters: {'reg_alpha': 1.0334942649292809, 'reg_lambda': 2.6068659700483607, 'colsample_bytree': 0.3, 'subsample': 0.8, 'learning_rate': 0.008, 'max_depth': 11, 'num_leaves': 321, 'min_child_samples': 166, 'min_data_per_groups': 26}. Best is trial 3 with value: 12.874837014299866.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-22 00:02:40,144]\u001b[0m Trial 7 finished with value: 14.030383601473652 and parameters: {'reg_alpha': 0.05940454432277137, 'reg_lambda': 0.01578786999522909, 'colsample_bytree': 0.3, 'subsample': 0.7, 'learning_rate': 0.05, 'max_depth': 7, 'num_leaves': 958, 'min_child_samples': 195, 'min_data_per_groups': 5}. Best is trial 3 with value: 12.874837014299866.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-22 00:09:21,279]\u001b[0m Trial 8 finished with value: 13.331040618874129 and parameters: {'reg_alpha': 0.4371072931104937, 'reg_lambda': 2.407594322503503, 'colsample_bytree': 1.0, 'subsample': 0.7, 'learning_rate': 0.005, 'max_depth': 11, 'num_leaves': 315, 'min_child_samples': 9, 'min_data_per_groups': 53}. Best is trial 3 with value: 12.874837014299866.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-22 00:10:40,935]\u001b[0m Trial 9 finished with value: 13.474963585356619 and parameters: {'reg_alpha': 6.615337557837484, 'reg_lambda': 0.023117155524174225, 'colsample_bytree': 0.7, 'subsample': 0.7, 'learning_rate': 0.01, 'max_depth': 9, 'num_leaves': 130, 'min_child_samples': 191, 'min_data_per_groups': 55}. Best is trial 3 with value: 12.874837014299866.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-22 00:11:25,703]\u001b[0m Trial 10 finished with value: 12.97252924101406 and parameters: {'reg_alpha': 0.22026348733553602, 'reg_lambda': 0.18263238204419233, 'colsample_bytree': 0.6, 'subsample': 1.0, 'learning_rate': 0.005, 'max_depth': 5, 'num_leaves': 706, 'min_child_samples': 89, 'min_data_per_groups': 30}. Best is trial 3 with value: 12.874837014299866.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-22 00:12:11,660]\u001b[0m Trial 11 finished with value: 12.973000022543877 and parameters: {'reg_alpha': 0.1680600074336294, 'reg_lambda': 0.18079098106068195, 'colsample_bytree': 0.6, 'subsample': 1.0, 'learning_rate': 0.005, 'max_depth': 5, 'num_leaves': 741, 'min_child_samples': 81, 'min_data_per_groups': 33}. Best is trial 3 with value: 12.874837014299866.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-22 00:12:59,273]\u001b[0m Trial 12 finished with value: 13.006836961601085 and parameters: {'reg_alpha': 0.03351819607970625, 'reg_lambda': 0.14617570102971766, 'colsample_bytree': 0.8, 'subsample': 1.0, 'learning_rate': 0.005, 'max_depth': 5, 'num_leaves': 982, 'min_child_samples': 97, 'min_data_per_groups': 35}. Best is trial 3 with value: 12.874837014299866.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-22 00:13:39,299]\u001b[0m Trial 13 finished with value: 12.955981925348897 and parameters: {'reg_alpha': 0.2517679101391687, 'reg_lambda': 0.4150008070116874, 'colsample_bytree': 0.4, 'subsample': 0.5, 'learning_rate': 0.005, 'max_depth': 5, 'num_leaves': 713, 'min_child_samples': 59, 'min_data_per_groups': 22}. Best is trial 3 with value: 12.874837014299866.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-22 00:14:27,392]\u001b[0m Trial 14 finished with value: 13.753157370840349 and parameters: {'reg_alpha': 0.008866762292022769, 'reg_lambda': 0.6028499735506144, 'colsample_bytree': 0.4, 'subsample': 0.5, 'learning_rate': 0.03, 'max_depth': 5, 'num_leaves': 432, 'min_child_samples': 2, 'min_data_per_groups': 19}. Best is trial 3 with value: 12.874837014299866.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-22 00:15:04,375]\u001b[0m Trial 15 finished with value: 12.8644994871053 and parameters: {'reg_alpha': 0.47306674673133675, 'reg_lambda': 0.04891753032348945, 'colsample_bytree': 0.4, 'subsample': 0.5, 'learning_rate': 0.005, 'max_depth': 5, 'num_leaves': 798, 'min_child_samples': 282, 'min_data_per_groups': 41}. Best is trial 15 with value: 12.8644994871053.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-22 00:15:40,441]\u001b[0m Trial 16 finished with value: 12.858951411637861 and parameters: {'reg_alpha': 1.2054963528918952, 'reg_lambda': 0.044607751065234444, 'colsample_bytree': 0.4, 'subsample': 0.5, 'learning_rate': 0.005, 'max_depth': 5, 'num_leaves': 867, 'min_child_samples': 291, 'min_data_per_groups': 75}. Best is trial 16 with value: 12.858951411637861.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-22 00:16:16,368]\u001b[0m Trial 17 finished with value: 12.844469658105648 and parameters: {'reg_alpha': 0.9250102888963972, 'reg_lambda': 0.0012070644986337915, 'colsample_bytree': 0.4, 'subsample': 0.5, 'learning_rate': 0.005, 'max_depth': 5, 'num_leaves': 822, 'min_child_samples': 294, 'min_data_per_groups': 82}. Best is trial 17 with value: 12.844469658105648.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-22 00:17:39,231]\u001b[0m Trial 18 finished with value: 13.993905174011061 and parameters: {'reg_alpha': 1.580563957764309, 'reg_lambda': 0.0015106950375849719, 'colsample_bytree': 0.9, 'subsample': 0.5, 'learning_rate': 0.03, 'max_depth': 9, 'num_leaves': 498, 'min_child_samples': 300, 'min_data_per_groups': 86}. Best is trial 17 with value: 12.844469658105648.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2738468821.py:9: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[32m[I 2022-11-22 00:18:32,277]\u001b[0m Trial 19 finished with value: 14.04550743508735 and parameters: {'reg_alpha': 0.7417538224406888, 'reg_lambda': 0.0012795007023266671, 'colsample_bytree': 0.5, 'subsample': 0.5, 'learning_rate': 0.05, 'max_depth': 7, 'num_leaves': 867, 'min_child_samples': 261, 'min_data_per_groups': 76}. Best is trial 17 with value: 12.844469658105648.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "lgbm_study = optuna.create_study(direction='minimize')\n",
    "lgbm_study.optimize(lgbm_objective, n_trials=num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba0ee545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reg_alpha': 0.9250102888963972,\n",
       " 'reg_lambda': 0.0012070644986337915,\n",
       " 'colsample_bytree': 0.4,\n",
       " 'subsample': 0.5,\n",
       " 'learning_rate': 0.005,\n",
       " 'max_depth': 5,\n",
       " 'num_leaves': 822,\n",
       " 'min_child_samples': 294,\n",
       " 'min_data_per_groups': 82,\n",
       " 'random_state': 42,\n",
       " 'metric': 'rmse',\n",
       " 'n_estimators': 20000}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_params=lgbm_study.best_params \n",
    "lgbm_params['random_state'] = 42\n",
    "lgbm_params['metric'] = 'rmse'\n",
    "lgbm_params['n_estimators'] = 20000\n",
    "lgbm_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62f6c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_objective(trial,data=X,target=y):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X , y, test_size=0.30, random_state=42)\n",
    "    param = {\n",
    "            'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "            'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "            'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
    "            'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n",
    "            'learning_rate': trial.suggest_categorical('learning_rate', [0.005,0.008,0.01,0.03,0.05]),\n",
    "            'n_estimators': 10000,\n",
    "            'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13]),\n",
    "            'random_state': 42,\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 300)\n",
    "        }\n",
    "    model = XGBRegressor(**param)  \n",
    "    \n",
    "    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=100,verbose=False)\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    rmse = mean_squared_error(y_test, preds,squared=False)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40fcb6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-22 00:18:32,315]\u001b[0m A new study created in memory with name: no-name-f4230263-de4c-4905-9ecc-9d166c5ec20c\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-11-22 00:19:25,709]\u001b[0m Trial 0 finished with value: 12.737263987377524 and parameters: {'lambda': 0.0055266843263541325, 'alpha': 8.618503615919849, 'colsample_bytree': 0.5, 'subsample': 1.0, 'learning_rate': 0.008, 'max_depth': 11, 'min_child_weight': 129}. Best is trial 0 with value: 12.737263987377524.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-11-22 00:20:20,910]\u001b[0m Trial 1 finished with value: 12.809570037657888 and parameters: {'lambda': 0.07201813902217838, 'alpha': 0.07172469836950145, 'colsample_bytree': 0.8, 'subsample': 0.4, 'learning_rate': 0.008, 'max_depth': 11, 'min_child_weight': 137}. Best is trial 0 with value: 12.737263987377524.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-11-22 00:20:42,101]\u001b[0m Trial 2 finished with value: 12.73944989007996 and parameters: {'lambda': 2.6029887915791634, 'alpha': 7.340487101928328, 'colsample_bytree': 1.0, 'subsample': 1.0, 'learning_rate': 0.03, 'max_depth': 9, 'min_child_weight': 82}. Best is trial 0 with value: 12.737263987377524.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-11-22 00:20:53,848]\u001b[0m Trial 3 finished with value: 12.829941873020536 and parameters: {'lambda': 0.06527987592132851, 'alpha': 0.0010036985552752443, 'colsample_bytree': 0.5, 'subsample': 0.7, 'learning_rate': 0.03, 'max_depth': 11, 'min_child_weight': 25}. Best is trial 0 with value: 12.737263987377524.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-11-22 00:22:11,234]\u001b[0m Trial 4 finished with value: 12.7949343557012 and parameters: {'lambda': 5.3465350100552405, 'alpha': 0.7488835786492363, 'colsample_bytree': 0.6, 'subsample': 0.8, 'learning_rate': 0.005, 'max_depth': 13, 'min_child_weight': 4}. Best is trial 0 with value: 12.737263987377524.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-11-22 00:23:05,420]\u001b[0m Trial 5 finished with value: 12.734856796697185 and parameters: {'lambda': 0.006754733595066913, 'alpha': 0.09897874633226152, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.008, 'max_depth': 7, 'min_child_weight': 52}. Best is trial 5 with value: 12.734856796697185.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-22 00:24:33,633]\u001b[0m Trial 6 finished with value: 12.714415663613222 and parameters: {'lambda': 2.4827990514474725, 'alpha': 0.31216401684148964, 'colsample_bytree': 0.5, 'subsample': 1.0, 'learning_rate': 0.005, 'max_depth': 7, 'min_child_weight': 132}. Best is trial 6 with value: 12.714415663613222.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-11-22 00:25:48,911]\u001b[0m Trial 7 finished with value: 12.785449339209736 and parameters: {'lambda': 0.1263283111190111, 'alpha': 4.786011542553362, 'colsample_bytree': 0.8, 'subsample': 0.7, 'learning_rate': 0.008, 'max_depth': 11, 'min_child_weight': 178}. Best is trial 6 with value: 12.714415663613222.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-11-22 00:27:08,815]\u001b[0m Trial 8 finished with value: 12.862432185891205 and parameters: {'lambda': 0.004509775769372476, 'alpha': 0.001602169091870955, 'colsample_bytree': 0.9, 'subsample': 0.4, 'learning_rate': 0.008, 'max_depth': 9, 'min_child_weight': 292}. Best is trial 6 with value: 12.714415663613222.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-11-22 00:27:54,952]\u001b[0m Trial 9 finished with value: 12.826344031982076 and parameters: {'lambda': 0.11088919035410573, 'alpha': 0.007432758673290172, 'colsample_bytree': 0.8, 'subsample': 0.4, 'learning_rate': 0.01, 'max_depth': 11, 'min_child_weight': 149}. Best is trial 6 with value: 12.714415663613222.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-11-22 00:28:08,081]\u001b[0m Trial 10 finished with value: 12.84453712983814 and parameters: {'lambda': 0.7947619169135896, 'alpha': 0.4066072152154333, 'colsample_bytree': 0.4, 'subsample': 0.5, 'learning_rate': 0.05, 'max_depth': 5, 'min_child_weight': 238}. Best is trial 6 with value: 12.714415663613222.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-11-22 00:29:39,127]\u001b[0m Trial 11 finished with value: 12.739296803576584 and parameters: {'lambda': 0.015877434443796683, 'alpha': 0.058218413429897656, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.005, 'max_depth': 7, 'min_child_weight': 76}. Best is trial 6 with value: 12.714415663613222.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-11-22 00:30:38,135]\u001b[0m Trial 12 finished with value: 12.746844969857918 and parameters: {'lambda': 0.6368414246520789, 'alpha': 0.31587339336533166, 'colsample_bytree': 0.3, 'subsample': 0.6, 'learning_rate': 0.005, 'max_depth': 7, 'min_child_weight': 73}. Best is trial 6 with value: 12.714415663613222.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-22 00:31:26,292]\u001b[0m Trial 13 finished with value: 12.7748771842671 and parameters: {'lambda': 0.0010267524397943792, 'alpha': 0.01866564396023856, 'colsample_bytree': 0.7, 'subsample': 1.0, 'learning_rate': 0.01, 'max_depth': 7, 'min_child_weight': 202}. Best is trial 6 with value: 12.714415663613222.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-11-22 00:31:37,409]\u001b[0m Trial 14 finished with value: 12.753208306566869 and parameters: {'lambda': 9.319426028294739, 'alpha': 0.9189389330827701, 'colsample_bytree': 0.7, 'subsample': 0.6, 'learning_rate': 0.05, 'max_depth': 7, 'min_child_weight': 102}. Best is trial 6 with value: 12.714415663613222.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-11-22 00:32:41,630]\u001b[0m Trial 15 finished with value: 12.726259951124264 and parameters: {'lambda': 0.5314318035122557, 'alpha': 0.1514091036662879, 'colsample_bytree': 0.5, 'subsample': 0.5, 'learning_rate': 0.005, 'max_depth': 7, 'min_child_weight': 41}. Best is trial 6 with value: 12.714415663613222.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-11-22 00:33:47,149]\u001b[0m Trial 16 finished with value: 12.729022384860503 and parameters: {'lambda': 0.820988212686695, 'alpha': 2.0482977380072533, 'colsample_bytree': 0.5, 'subsample': 0.5, 'learning_rate': 0.005, 'max_depth': 13, 'min_child_weight': 34}. Best is trial 6 with value: 12.714415663613222.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-11-22 00:35:20,226]\u001b[0m Trial 17 finished with value: 12.790982263597119 and parameters: {'lambda': 2.064700045753518, 'alpha': 0.02132520886597068, 'colsample_bytree': 0.5, 'subsample': 0.5, 'learning_rate': 0.005, 'max_depth': 5, 'min_child_weight': 196}. Best is trial 6 with value: 12.714415663613222.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-11-22 00:36:30,334]\u001b[0m Trial 18 finished with value: 12.744729304809722 and parameters: {'lambda': 0.25780386793807186, 'alpha': 0.26927409014252224, 'colsample_bytree': 0.5, 'subsample': 0.8, 'learning_rate': 0.005, 'max_depth': 7, 'min_child_weight': 108}. Best is trial 6 with value: 12.714415663613222.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\2576784288.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:861: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2022-11-22 00:37:52,740]\u001b[0m Trial 19 finished with value: 12.758577149610717 and parameters: {'lambda': 0.33841509539804593, 'alpha': 0.1798284597881414, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.005, 'max_depth': 7, 'min_child_weight': 252}. Best is trial 6 with value: 12.714415663613222.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "xgb_study = optuna.create_study(direction='minimize')\n",
    "xgb_study.optimize(xgb_objective, n_trials=num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a34413a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lambda': 2.4827990514474725,\n",
       " 'alpha': 0.31216401684148964,\n",
       " 'colsample_bytree': 0.5,\n",
       " 'subsample': 1.0,\n",
       " 'learning_rate': 0.005,\n",
       " 'max_depth': 7,\n",
       " 'min_child_weight': 132,\n",
       " 'random_state': 42,\n",
       " 'metric': 'rmse',\n",
       " 'n_estimators': 10000}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_params=xgb_study.best_params \n",
    "xgb_params['random_state'] = 42\n",
    "xgb_params['metric'] = 'rmse'\n",
    "xgb_params['n_estimators'] = 10000\n",
    "xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "974f5945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_objective(trial,data=X,target=y):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X , y, test_size=0.30, random_state=42)\n",
    "    param = {\n",
    "        'loss_function': 'RMSE',\n",
    "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
    "        'max_bin': trial.suggest_int('max_bin', 200, 400),\n",
    "        #'rsm': trial.suggest_uniform('rsm', 0.3, 1.0),\n",
    "        'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "        'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
    "        'n_estimators':  25000,\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [5,7,9]),\n",
    "        'random_state': 42,\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 300),\n",
    "    }\n",
    "    model = CatBoostRegressor(**param) \n",
    "    \n",
    "    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=20,verbose=False)\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    rmse = mean_squared_error(y_test, preds,squared=False)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ab70759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-22 00:37:52,800]\u001b[0m A new study created in memory with name: no-name-3693f7cf-b340-415a-83c8-d0f26d197fb7\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
      "\u001b[32m[I 2022-11-22 00:40:16,318]\u001b[0m Trial 0 finished with value: 12.675243810660426 and parameters: {'l2_leaf_reg': 1.5324012560589697, 'max_bin': 369, 'bagging_fraction': 0.40089440581806846, 'learning_rate': 0.006657375481317432, 'max_depth': 9, 'min_data_in_leaf': 77}. Best is trial 0 with value: 12.675243810660426.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
      "\u001b[32m[I 2022-11-22 00:40:43,510]\u001b[0m Trial 1 finished with value: 12.717311898434797 and parameters: {'l2_leaf_reg': 0.19469841922507647, 'max_bin': 293, 'bagging_fraction': 0.9643924380686274, 'learning_rate': 0.009214770669859691, 'max_depth': 7, 'min_data_in_leaf': 38}. Best is trial 0 with value: 12.675243810660426.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
      "\u001b[32m[I 2022-11-22 00:40:58,404]\u001b[0m Trial 2 finished with value: 12.745380130714993 and parameters: {'l2_leaf_reg': 0.3819069015607005, 'max_bin': 299, 'bagging_fraction': 0.441424046580326, 'learning_rate': 0.011550453869138059, 'max_depth': 5, 'min_data_in_leaf': 166}. Best is trial 0 with value: 12.675243810660426.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
      "\u001b[32m[I 2022-11-22 00:41:58,905]\u001b[0m Trial 3 finished with value: 12.665553990728592 and parameters: {'l2_leaf_reg': 2.469506209511148, 'max_bin': 270, 'bagging_fraction': 0.9399264123108971, 'learning_rate': 0.01341502639803436, 'max_depth': 9, 'min_data_in_leaf': 147}. Best is trial 3 with value: 12.665553990728592.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
      "\u001b[32m[I 2022-11-22 00:42:30,098]\u001b[0m Trial 4 finished with value: 12.722552213798782 and parameters: {'l2_leaf_reg': 0.017400636216525597, 'max_bin': 326, 'bagging_fraction': 0.7396777555097225, 'learning_rate': 0.007217385469897366, 'max_depth': 7, 'min_data_in_leaf': 149}. Best is trial 3 with value: 12.665553990728592.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
      "\u001b[32m[I 2022-11-22 00:42:40,060]\u001b[0m Trial 5 finished with value: 12.776987818294963 and parameters: {'l2_leaf_reg': 5.293010788724649, 'max_bin': 250, 'bagging_fraction': 0.9909900464154547, 'learning_rate': 0.017504949658290497, 'max_depth': 5, 'min_data_in_leaf': 248}. Best is trial 3 with value: 12.665553990728592.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
      "\u001b[32m[I 2022-11-22 00:44:12,420]\u001b[0m Trial 6 finished with value: 12.671623582430236 and parameters: {'l2_leaf_reg': 8.524349109082165, 'max_bin': 250, 'bagging_fraction': 0.4151862780736314, 'learning_rate': 0.010232499376767375, 'max_depth': 9, 'min_data_in_leaf': 97}. Best is trial 3 with value: 12.665553990728592.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
      "\u001b[32m[I 2022-11-22 00:44:30,036]\u001b[0m Trial 7 finished with value: 12.714489976256365 and parameters: {'l2_leaf_reg': 0.013822952073397779, 'max_bin': 355, 'bagging_fraction': 0.51353695001435, 'learning_rate': 0.014940393529450625, 'max_depth': 7, 'min_data_in_leaf': 202}. Best is trial 3 with value: 12.665553990728592.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
      "\u001b[32m[I 2022-11-22 00:44:50,062]\u001b[0m Trial 8 finished with value: 12.720388615675489 and parameters: {'l2_leaf_reg': 0.0032328774696706425, 'max_bin': 306, 'bagging_fraction': 0.8173697561249563, 'learning_rate': 0.012249178536292621, 'max_depth': 7, 'min_data_in_leaf': 212}. Best is trial 3 with value: 12.665553990728592.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
      "\u001b[32m[I 2022-11-22 00:46:02,337]\u001b[0m Trial 9 finished with value: 12.676866901770714 and parameters: {'l2_leaf_reg': 0.025222485866808444, 'max_bin': 252, 'bagging_fraction': 0.9419532435624589, 'learning_rate': 0.0071323217495856215, 'max_depth': 9, 'min_data_in_leaf': 208}. Best is trial 3 with value: 12.665553990728592.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
      "\u001b[32m[I 2022-11-22 00:46:38,399]\u001b[0m Trial 10 finished with value: 12.68148576161666 and parameters: {'l2_leaf_reg': 0.6491475209465466, 'max_bin': 213, 'bagging_fraction': 0.6623600556693248, 'learning_rate': 0.01427384480766197, 'max_depth': 9, 'min_data_in_leaf': 287}. Best is trial 3 with value: 12.665553990728592.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
      "\u001b[32m[I 2022-11-22 00:48:14,356]\u001b[0m Trial 11 finished with value: 12.65037757185054 and parameters: {'l2_leaf_reg': 8.71324273361747, 'max_bin': 252, 'bagging_fraction': 0.6136999924819322, 'learning_rate': 0.010755732567429425, 'max_depth': 9, 'min_data_in_leaf': 100}. Best is trial 11 with value: 12.65037757185054.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
      "\u001b[32m[I 2022-11-22 00:48:55,360]\u001b[0m Trial 12 finished with value: 12.65814572726186 and parameters: {'l2_leaf_reg': 1.9867831299571104, 'max_bin': 208, 'bagging_fraction': 0.6151209472605381, 'learning_rate': 0.013725928782790275, 'max_depth': 9, 'min_data_in_leaf': 117}. Best is trial 11 with value: 12.65037757185054.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
      "\u001b[32m[I 2022-11-22 00:49:45,342]\u001b[0m Trial 13 finished with value: 12.653563899620876 and parameters: {'l2_leaf_reg': 8.888957053421688, 'max_bin': 202, 'bagging_fraction': 0.6018625894800803, 'learning_rate': 0.01657933924455047, 'max_depth': 9, 'min_data_in_leaf': 112}. Best is trial 11 with value: 12.65037757185054.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
      "\u001b[32m[I 2022-11-22 00:50:43,576]\u001b[0m Trial 14 finished with value: 12.639952511726063 and parameters: {'l2_leaf_reg': 9.69076342640467, 'max_bin': 224, 'bagging_fraction': 0.5555361771498785, 'learning_rate': 0.017642646762641698, 'max_depth': 9, 'min_data_in_leaf': 18}. Best is trial 14 with value: 12.639952511726063.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
      "\u001b[32m[I 2022-11-22 00:51:34,126]\u001b[0m Trial 15 finished with value: 12.67893981241964 and parameters: {'l2_leaf_reg': 0.0642818104884443, 'max_bin': 228, 'bagging_fraction': 0.5347144687235481, 'learning_rate': 0.009393936963859907, 'max_depth': 9, 'min_data_in_leaf': 7}. Best is trial 14 with value: 12.639952511726063.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
      "\u001b[32m[I 2022-11-22 00:51:44,429]\u001b[0m Trial 16 finished with value: 12.753959515862817 and parameters: {'l2_leaf_reg': 0.0012315740910160834, 'max_bin': 234, 'bagging_fraction': 0.7491401175106466, 'learning_rate': 0.016228878101801018, 'max_depth': 5, 'min_data_in_leaf': 53}. Best is trial 14 with value: 12.639952511726063.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
      "\u001b[32m[I 2022-11-22 00:52:44,543]\u001b[0m Trial 17 finished with value: 12.665534528693957 and parameters: {'l2_leaf_reg': 0.7716848866130864, 'max_bin': 275, 'bagging_fraction': 0.5202845863153951, 'learning_rate': 0.011282479224128181, 'max_depth': 9, 'min_data_in_leaf': 20}. Best is trial 14 with value: 12.639952511726063.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
      "\u001b[32m[I 2022-11-22 00:54:10,230]\u001b[0m Trial 18 finished with value: 12.647234572075 and parameters: {'l2_leaf_reg': 3.7687885410207613, 'max_bin': 227, 'bagging_fraction': 0.8129137633753698, 'learning_rate': 0.00836521458262714, 'max_depth': 9, 'min_data_in_leaf': 67}. Best is trial 14 with value: 12.639952511726063.\u001b[0m\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'subsample': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
      "C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\742444196.py:10: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_uniform('learning_rate', 0.006, 0.018),\n",
      "\u001b[32m[I 2022-11-22 00:54:30,898]\u001b[0m Trial 19 finished with value: 12.771006239422356 and parameters: {'l2_leaf_reg': 2.838553149695382, 'max_bin': 398, 'bagging_fraction': 0.8161793448010437, 'learning_rate': 0.00841211735736041, 'max_depth': 5, 'min_data_in_leaf': 55}. Best is trial 14 with value: 12.639952511726063.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cat_study = optuna.create_study(direction='minimize')\n",
    "cat_study.optimize(cat_objective, n_trials=num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10a37c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l2_leaf_reg': 9.69076342640467,\n",
       " 'max_bin': 224,\n",
       " 'learning_rate': 0.017642646762641698,\n",
       " 'max_depth': 9,\n",
       " 'min_data_in_leaf': 18,\n",
       " 'random_state': 42,\n",
       " 'loss_function': 'RMSE',\n",
       " 'n_estimators': 25000}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_params=cat_study.best_params \n",
    "cat_params['random_state'] = 42\n",
    "cat_params['loss_function'] = 'RMSE'\n",
    "cat_params['n_estimators'] = 25000\n",
    "cat_params.pop('bagging_fraction')\n",
    "cat_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95c6ea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbr_objective(trial):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X , y, test_size=0.30, random_state=42)\n",
    "    param = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 1000, 10000, step=200),\n",
    "        \"learning_rate\": trial.suggest_categorical('learning_rate', [0.01,0.03,0.05,0.1]),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 9, step=2),\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "    model = GradientBoostingRegressor(**param)\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    rmse = mean_squared_error(y_test, preds,squared=False)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0117be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-22 00:54:30,942]\u001b[0m A new study created in memory with name: no-name-690d5218-303a-4da3-a64b-adefefe12d94\u001b[0m\n",
      "\u001b[33m[W 2022-11-22 01:06:24,668]\u001b[0m Trial 0 failed because of the following error: KeyboardInterrupt()\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\bhara\\AppData\\Local\\Temp\\ipykernel_19880\\3902487942.py\", line 10, in gbr_objective\n",
      "    model.fit(X_train,y_train)\n",
      "  File \"C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 668, in fit\n",
      "    n_stages = self._fit_stages(\n",
      "  File \"C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 745, in _fit_stages\n",
      "    raw_predictions = self._fit_stage(\n",
      "  File \"C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 247, in _fit_stage\n",
      "    tree.fit(X, residual, sample_weight=sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1342, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\bhara\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 458, in fit\n",
      "    builder.build(self.tree_, X, y, sample_weight)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m gbr_study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mgbr_study\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgbr_objective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    324\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    230\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    233\u001b[0m ):\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mgbr_objective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m      3\u001b[0m param \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m10000\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m),\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_categorical(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;241m0.01\u001b[39m,\u001b[38;5;241m0.03\u001b[39m,\u001b[38;5;241m0.05\u001b[39m,\u001b[38;5;241m0.1\u001b[39m]),\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m\"\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m9\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m      8\u001b[0m }\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m GradientBoostingRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparam)\n\u001b[1;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     14\u001b[0m rmse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_test, preds,squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:668\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    667\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 668\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:745\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    738\u001b[0m     old_oob_score \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[0;32m    739\u001b[0m         y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    740\u001b[0m         raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    741\u001b[0m         sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    742\u001b[0m     )\n\u001b[0;32m    744\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;66;03m# track deviance (= loss)\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:247\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    244\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    246\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 247\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    250\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[0;32m    251\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[0;32m    252\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    259\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    260\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:1342\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m \n\u001b[0;32m   1316\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1342\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1345\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:458\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    448\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    449\u001b[0m         splitter,\n\u001b[0;32m    450\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    456\u001b[0m     )\n\u001b[1;32m--> 458\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gbr_study = optuna.create_study(direction='minimize')\n",
    "gbr_study.optimize(gbr_objective, n_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdec750",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_params=gbr_study.best_params \n",
    "gbr_params['random_state'] = 42\n",
    "gbr_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5988e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_objective(trial):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X , y, test_size=0.30, random_state=42)\n",
    "    param = {\n",
    "        \"n_estimators\": trial.suggest_int(name=\"n_estimators\", low=100, high=1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8),\n",
    "        \"max_features\": trial.suggest_categorical(\n",
    "            \"max_features\", choices=[\"auto\", \"sqrt\", \"log2\"]\n",
    "        ),\n",
    "        \"n_jobs\": -1,\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "\n",
    "    model = RandomForestRegressor(**param)\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    rmse = mean_squared_error(y_test, preds,squared=False)\n",
    "    \n",
    "    return rmse\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d8d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_study = optuna.create_study(direction='minimize')\n",
    "rf_study.optimize(rf_objective, n_trials=num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d72637",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params=rf_study.best_params \n",
    "rf_params['random_state'] = 42\n",
    "rf_params['n_jobs'] = -1\n",
    "rf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59826a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svr_objective(trial):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X , y, test_size=0.30, random_state=42)\n",
    "    param = {\n",
    "        'C': trial.suggest_loguniform('C', 0.3, 10.0),\n",
    "        'epsilon': trial.suggest_float('epsilon', 0.05, 0.5),\n",
    "    }\n",
    "    \n",
    "    model = SVR(**param)\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    rmse = mean_squared_error(y_test, preds,squared=False)\n",
    "    \n",
    "    return rmse\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf57229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_study = optuna.create_study(direction='minimize')\n",
    "svr_study.optimize(svr_objective, n_trials=num_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945f353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_params=svr_study.best_params \n",
    "#svr_params['random_state'] = 42\n",
    "#svr_params['n_jobs'] = -1\n",
    "svr_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6278c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hello World!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cb8e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(**xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e77cadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:07:49] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-03de431ba26204c4d-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"metric\" } are not used.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(alpha=0.31216401684148964, base_score=0.5, booster=&#x27;gbtree&#x27;,\n",
       "             callbacks=None, colsample_bylevel=1, colsample_bynode=1,\n",
       "             colsample_bytree=0.5, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;, importance_type=None,\n",
       "             interaction_constraints=&#x27;&#x27;, lambda=2.4827990514474725,\n",
       "             learning_rate=0.005, max_bin=256, max_cat_threshold=64,\n",
       "             max_cat_to_onehot=4, max_delta_step=0, max_depth=7, max_leaves=0,\n",
       "             metric=&#x27;rmse&#x27;, min_child_weight=132, missing=nan,\n",
       "             monotone_constraints=&#x27;()&#x27;, n_estimators=10000, n_jobs=0, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(alpha=0.31216401684148964, base_score=0.5, booster=&#x27;gbtree&#x27;,\n",
       "             callbacks=None, colsample_bylevel=1, colsample_bynode=1,\n",
       "             colsample_bytree=0.5, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;, importance_type=None,\n",
       "             interaction_constraints=&#x27;&#x27;, lambda=2.4827990514474725,\n",
       "             learning_rate=0.005, max_bin=256, max_cat_threshold=64,\n",
       "             max_cat_to_onehot=4, max_delta_step=0, max_depth=7, max_leaves=0,\n",
       "             metric=&#x27;rmse&#x27;, min_child_weight=132, missing=nan,\n",
       "             monotone_constraints=&#x27;()&#x27;, n_estimators=10000, n_jobs=0, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(alpha=0.31216401684148964, base_score=0.5, booster='gbtree',\n",
       "             callbacks=None, colsample_bylevel=1, colsample_bynode=1,\n",
       "             colsample_bytree=0.5, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=0, gpu_id=-1, grow_policy='depthwise', importance_type=None,\n",
       "             interaction_constraints='', lambda=2.4827990514474725,\n",
       "             learning_rate=0.005, max_bin=256, max_cat_threshold=64,\n",
       "             max_cat_to_onehot=4, max_delta_step=0, max_depth=7, max_leaves=0,\n",
       "             metric='rmse', min_child_weight=132, missing=nan,\n",
       "             monotone_constraints='()', n_estimators=10000, n_jobs=0, ...)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2812282",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d276d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = inv_boxcox(y_pred, y_train_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c63aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "this == that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f99bbe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('sample_submission.csv')\n",
    "sub = pd.DataFrame({'id': sub_df.id, 'price_doc': y_pred})\n",
    "sub.to_csv('sub004.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf6736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8caacf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "level0 = list()\n",
    "level0.append(('xgb', XGBRegressor(**xgb_params)))\n",
    "level0.append(('lgbm', LGBMRegressor(**lgbm_params)))\n",
    "level0.append(('svm', SVR(**svr_params)))\n",
    "level0.append(('gbr', GradientBoostingRegressor(**gbr_params)))\n",
    "level0.append(('rf', RandomForestRegressor(**rf_params)))\n",
    "level0.append(('cat', CatBoostRegressor(**cat_params)))\n",
    "level1 = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bca32a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a492410",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dffbe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee74d167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
